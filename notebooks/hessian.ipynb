{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae42914a-8244-45cd-80bf-802c58ecab48",
   "metadata": {},
   "source": [
    "# Efficient Batched Hessian Calculation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb11f7-bfbb-44f7-81ae-0f41c50d9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977d2f5-8d63-4fcb-acc8-11b5f7f072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hessian import hessian, jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c3f91-544f-4d25-97f0-b2b021453ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using PyTorch version: {t.__version__}')\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256e22c-afb4-4dde-a8d6-853d3fee99bc",
   "metadata": {},
   "source": [
    "To get started, here is a naive way of computing the jacobian of a vector-valued function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76ef1c-2157-47e5-8e22-69f6fc298833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(weight, bias, x):\n",
    "    return F.linear(x, weight, bias).tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f66992-ad17-4a32-8290-094d0cff8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 16\n",
    "weight = t.randn(D, D)  # affine mapping R^D -> R^D\n",
    "bias = t.randn(D)\n",
    "x = t.randn(D)  # feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e0195-d04b-4b5a-ae41-2f4b7ad4ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(xp):\n",
    "    assert xp.dim() == 1\n",
    "    unit_vectors = t.eye(xp.size(0))\n",
    "    jacobian_rows = [t.autograd.grad(f(weight, bias, xp), xp, vec)[0]\n",
    "                     for vec in unit_vectors]\n",
    "    return t.stack(jacobian_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7bb68-405d-4efa-a4dd-c66ba55adb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = x.clone().requires_grad_()\n",
    "print(f'Input shape: {xp.shape}')\n",
    "my_jacobian = compute_jacobian(xp)\n",
    "print(f'Jacobian shape: {my_jacobian.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ecaec-a403-4a12-8908-eecbe231e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "newxp = xp.clone().requires_grad_()\n",
    "outputs = f(weight, bias, newxp)\n",
    "lib_jacobian = jacobian(outputs, newxp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaea4fb-1b7a-48ab-a51d-9236d2d34b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.isclose(my_jacobian, lib_jacobian).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94a716-23ee-4978-a4f1-70f5826b8f60",
   "metadata": {},
   "source": [
    "As we can see, the output of the above is a `[D, D]` matrix, with each row being the jacobian of the ith function value with respect to all inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e4580-898f-4618-99b2-fb422bb91d45",
   "metadata": {},
   "source": [
    "Computing the Jacobian row-by-row like this is very computationally inefficient, particularly with larger matrices.\n",
    "\n",
    "Rather than looping, we can vectorise the above using `vmap` and `vjp`: the vector-Jacobian product function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77995741-6fd2-4b8b-bf55-b94cfb16901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd.functional import vjp\n",
    "from functorch import vmap, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b113360-b81d-4228-ab8c-417678f19189",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vjp_fn = vjp(partial(f, weight, bias), x)\n",
    "ft_jacobian, = vmap(vjp_fn)(t.eye(x.size(0)))\n",
    "\n",
    "assert t.allclose(ft_jacobian, my_jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef686b97-d986-41bb-a83a-ea6c93d9abce",
   "metadata": {},
   "source": [
    "Functorch provides a handy alias to the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498f2e7-1751-4486-b6e0-4867dc54b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import jacrev\n",
    "ft_jacobian = jacrev(f, argnums=2)(weight, bias, x)\n",
    "\n",
    "assert t.allclose(ft_jacobian, jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58234d89-67ae-4ef9-85d1-10cdc51254cd",
   "metadata": {},
   "source": [
    "We can also flip the problem around and say we want to compute the Jacobians of the parameters to our model (i.e. the weight and bias terms), rather than the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f199f-31c3-48ab-a178-368d187abbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_jac_weight, ft_jac_bias = jacrev(f, argnums=(0, 1))(weight, bias, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d97dd2-9d13-44f9-8bf7-704afe70d146",
   "metadata": {},
   "source": [
    "Note that if we're computing the Jacobian of a $\\mathbb{R}^N \\to \\mathbb{R}^M$ function (where there are more outputs and inputs $M > N$), then `jacfwd` (the version of the above using forward-mode automatic differentiation) is preferred. Otherwise use `jacrev` which uses the usual AD.\n",
    "\n",
    "In reverse-mode AD, we compute the Jacobian row-by-row, while in forward-mode AD (which uses Jacobian-vector products), we are computing it column-by-column. Since the Jacobian matrix has $M$ rows and $N$ columns, if it is taller or wider one way, we might prefer the method that deals with fewer rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b32737-672d-48d3-a707-22f565167693",
   "metadata": {},
   "outputs": [],
   "source": [
    "Din = 32\n",
    "Dout = 2048\n",
    "weight = t.randn(Dout, Din)\n",
    "bias = t.randn(Dout)\n",
    "x = t.randn(Din)\n",
    "\n",
    "print(f'weight shape: {weight.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277be7b-edbc-40c7-84ea-b444997338b7",
   "metadata": {},
   "source": [
    "Here we seem to have a taller matrix (taller, that is, than it is wide). Hence, we should use forward mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be9558-52e8-4f6f-92e7-d06ef4599353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import jacfwd\n",
    "using_fwd = jacfwd(f, argnums=2)(weight, bias, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f3694-4354-4554-9408-9ad361dc74ab",
   "metadata": {},
   "source": [
    "If the function f gave fewer outputs than inputs, then we should use `jacrev`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666de523-9466-4a0b-91fe-f43b708a5e04",
   "metadata": {},
   "source": [
    "## Hessian Computation\n",
    "\n",
    "Recall that a Hessian is merely the Jacobian of the Jacobian; the matrix of second-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c858030-0000-468c-9f93-ec5ca4055973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functorch import hessian\n",
    "\n",
    "Din = 512\n",
    "Dout = 32\n",
    "weight = t.randn(Dout, Din)\n",
    "bias = t.randn(Dout)\n",
    "x = t.randn(Din)\n",
    "\n",
    "hess_api = hessian(f, argnums=2)(weight, bias, x)\n",
    "hess_revrev = jacrev(jacrev(f, argnums=2), argnums=2)(weight, bias, x)\n",
    "assert t.allclose(hess_api, hess_revrev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca864ce4-8772-4dde-aed2-6f4cf5ddf537",
   "metadata": {},
   "source": [
    "## Batched Computation\n",
    "\n",
    "In the examples above, we've been using singleton vectors. We usually want to take the Jacobian (and Hessian) of a batch of outputs with respect to a batch of inputs.\n",
    "\n",
    "Given a batch of inputs of shape `(B, N)` and a function $f: \\mathbb{R}^N \\to \\mathbb{R}^M$, we'd like a Jacobian of shape `(B, M, N)`.\n",
    "\n",
    "We can vectorise this operation using vmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd042e-0322-4336-a31d-0bcc33f9ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "Din = 31\n",
    "Dout = 33\n",
    "\n",
    "weight = t.randn(Dout, Din)\n",
    "bias = t.randn(Dout)\n",
    "print(f'f is a transformation from {weight.size(1)} to {weight.size(0)}')\n",
    "\n",
    "x = t.randn(batch_size, Din)\n",
    "print(f'Input batch is of size: {x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d026a-b0dc-4090-a699-340988082bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_jacobian_fn = vmap(jacrev(f, argnums=2), in_dims=(None, None, 0))\n",
    "batch_jacobian = batch_jacobian_fn(weight, bias, x)\n",
    "print(f'Resulting Jacobian is of size: {batch_jacobian.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e5a25-a2fa-402b-9a4b-8a74ddf0654d",
   "metadata": {},
   "source": [
    "We can compute batched Hessians in a similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243c052-32fc-471b-8878-19a2281bd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_hessian_fn = vmap(hessian(f, argnums=2), in_dims=(None, None, 0))\n",
    "batch_hess = batch_hessian_fn(weight, bias, x)\n",
    "print(f'Resulting Hessian is of size: {batch_hess.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c29ce-b065-465d-ac28-607cec6cfc5b",
   "metadata": {},
   "source": [
    "## Advanced usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cad92-ce72-4a95-88bf-4027ff4b5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "Din = 32\n",
    "Dout = 1\n",
    "\n",
    "weights = t.randn(Dout, Din)\n",
    "\n",
    "def critic(states, actions):\n",
    "    assert states.shape == (batch_size, Din)\n",
    "    assert actions.shape == (batch_size, Din)\n",
    "    tmp = states + actions.sum(1)[:, None]\n",
    "    return F.linear(tmp, weights).tanh(), F.linear(tmp, weights).tanh()\n",
    "\n",
    "def td_err(states, actions, q_target):\n",
    "    q1, q2 = critic(states, actions)\n",
    "    return q_target - q1, q_target - q2\n",
    "\n",
    "states = t.randn(batch_size, Din)\n",
    "actions = t.randn(batch_size, Din)\n",
    "q_target = t.randn(batch_size, Dout)\n",
    "\n",
    "a, b = jacrev(td_err, has_aux=True, argnums=(1, 1))(states, actions, actions, q_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b8c88-618f-48a8-a001-26b8b19888e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b266b7-30b3-4db0-8410-a72f9c8d79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, vjpfunc) = vjp(td_err, states, actions, q_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc5c2a-9d7d-42c7-85c2-28d6e3671f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa, bb = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3861bb-9e3d-40ca-9eff-b6c6a92a9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936e958-90c3-4872-9be8-a3f1805bdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857ecb8-f62c-41c5-a923-f7a9eb5d68da",
   "metadata": {},
   "source": [
    "(jac1, jac2), (td_err_1, td_err_2) = jacrev(td_err, has_aux=True, argnums=1)(states, actions, q_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taylor RL (Python 3.9)",
   "language": "python",
   "name": "taylor_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
